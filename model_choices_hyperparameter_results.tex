\chapter{Machine Learning Models Explored} \label{ChapterMachineLearningModelsExplored}

This section describes the process of choosing the architectural and regularization hyperparamers for the dense and 1D convolutional models explored in this work. This section also describes the datasets used to tune the hyperparameters.

% Dense and convolutional models are explored for two different reasons. 

\section{Training Templates Overview}

The datasets used to train the models are created using templates of simulated gamma-ray spectra without Poisson noise. A one-dimensional particle transport code developed at Sandia National Laboratory, GADRAS-DRF (Gamma Detector Response and Analysis Software - Detector Response Function), was used to generate these templates. This simulation code can be used to model physical conditions that affect spectra. For example, spectra change due to changes in measurement geometry. These geometry changes can include the source and detector distance from each other, the ground, and objects in the environment. To demonstrate two of these effects, $^{60}$Co spectra were simulated at various source-to-detector distances, Figure \ref{fig:sim_spectra_distance_comparison}, and heights off ground, Figure \ref{fig:sim_spectra_height_comparison}. These parameters change the shape of the Compton continuum, the peak-to-total ratio, and the magnitude of the backscatter peak. Another parameter that changes the spectrum is the Gaussian energy broadening of the photopeaks. Due to manufacturing differences, each detector has a different amount of energy broadening. Examples of $^{60}$Co spectra with different broadening parameters are shown in Figure \ref{fig:sim_spectra_FWHM_comparison}.


\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_distance_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated using various source-detector distances.}
\label{fig:sim_spectra_distance_comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_height_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated using various source-detector heights off the ground.}
\label{fig:sim_spectra_height_comparison}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_FWHM_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated with various FWHM parameters.}
\label{fig:sim_spectra_FWHM_comparison}
\end{figure}




% To incorporate these changes, templates simulated at different distances are included in the dataset. These distances start at 30cm, which is the distance at which a 1 uCi source will have an activity of about 400 cps on a 2 inch diameter detector. This is about twice the expected activity of background. 

% Changes in calibration due to temperature shifts are also considered. Due to the relatively large magnitude in calibration shifts due to temperature shifts from -5 C to 40 C \cite{CASANOVAS2012588}, it is expected incorporating these additions will also make the algorithm robust against calibration.

The template parameters used in this study are shown in Table \ref{table:all_fixed_simulation_parameters}. These parameters are based on handheld RIID scenarios. The source-detector height off the ground are sampled between shin and arm heights. The source-detector distance range corresponds to expected standoff distances involved with measuring a source in a cargo container. The areal density of each material corresponds to a 20$\%$, 40$\%$, 60$\%$, and 80$\%$ attenuation of a 200 keV photon. This photon energy was chosen because it is near the 186 keV energy of the characteristic $^{235}$U photopeak. A set of unshielded templates is also included. The FWHM range was chosen based on reported FWHM values for a NaI(Tl) detector (see section \ref{subsection_energy_resolution}). Background locations were chosen based on their geographic and geologic differences.

In addition to the parameters simulated by GADRAS-DRF, parameters are included to perform additional data augmentation. These parameters are shown in Table \ref{table:all_variable_simulation_parameters}.

\begin{table}[H]
\centering
\caption{Fixed GADRAS-DRF simulation parameters.}
\begin{tabular}{cc}
\hline
Simulation Parameter & Values \\ \hline
\begin{tabular}[c]{@{}c@{}}source-detector height \\ off ground {[}cm{]}\end{tabular} & 50, 75, 100, 125, 150 \\ 
\begin{tabular}[c]{@{}c@{}}source-detector\\ distance {[}cm{]}\end{tabular} & \multicolumn{1}{c}{50, 112.5, 175, 237.5, 300} \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of \\ solid aluminum {[}g/cm\textasciicircum{}2{]}\end{tabular} & 1.82, 4.18, 7.49, 13.16 \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of\\  solid iron {[}g/cm\textasciicircum{}2{]}\end{tabular} & 1.53, 3.5, 6.28, 11.02 \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of \\ solid lead {[}g/cm\textasciicircum{}2{]}\end{tabular} & 0.22, 0.51, 0.92, 1.61 \\ 
\begin{tabular}[c]{@{}c@{}}FWHM at\\  662 keV {[}\%{]}\end{tabular} & \multicolumn{1}{c}{6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0} \\ 
\begin{tabular}[c]{@{}c@{}}Background\\ Location \end{tabular} & \begin{tabular}[c]{@{}c@{}}Albuquerque, Atlanta, Austin,\\ Chicago , Knoxville , Miami\end{tabular} \\ \hline
\end{tabular}
\label{table:all_fixed_simulation_parameters}
\end{table}

\begin{table}[H]
\centering
\caption{Default data augmentation parameters.}
\begin{tabular}{c}
\hline
Parameter \\ \hline
integration time \\ 
background counts per second \\ 
Signal to Background ratio \\ 
Calibration - Offset \\ 
Calibration - Gain \\ \hline
\end{tabular}
\label{table:all_variable_simulation_parameters}
\end{table}

% \begin{table}[H]
% \centering
% \caption{Default data augmentation parameters.}
% \begin{tabular}{ccc}
% Parameter & Values & Sampling \\ \hline
% integration time & 60 - 600 & uniform \\ 
% background counts per second & 200 & Poisson \\ 
% Signal to Background ratio & 0.5 - 2 & uniform \\ 
% 0th order calibration & 0 - 10 & uniform \\ 
% 1st order calibration & 0.83 - 1.16 & uniform \\ 
% \end{tabular}
% \label{table:all_variable_simulation_parameters}
% \end{table}


\section{Datasets Used for the Hyperparameter Search} \label{datasets_used_hyperparam_search}

To investigate which parameters affect generalization performance, networks were trained using a simple and complete set of template simulation parameters. The parameters used for the simplified dataset are shown in Table \ref{table:hyperparameter_dataset_easy_parameters} and parameters used for the complete dataset are shown in Table \ref{table:hyperparameter_dataset_full_parameters}. Both datasets are comprised of pure background templates and the following isotopes from ANSI N42-34-2006 \cite{ANSI}: $^{241}$Am, $^{133}$Ba, $^{57}$Co, $^{60}$Co, $^{51}$Cr, $^{137}$Cs, $^{152}$Eu, $^{67}$Ga, $^{123}$I, $^{125}$I, $^{131}$I, $^{111}$In, $^{192}$Ir, $^{177m}$Lu, $^{99}$Mo, $^{237}$Np, $^{103}$Pd, $^{239}$Pu, $^{240}$Pu, $^{226}$Ra, $^{75}$Se, $^{153}$Sm, $^{99m}$Tc, $^{201}$Tl, $^{204}$Tl, $^{233}$U, $^{235}$U, $^{238}$U, and $^{133}$Xe.

Independent hyperparameter searches were conducted for both datasets because datasets of different complexity often need different architectures and hyperparameters \cite{Bergstra2012}. For each dataset, a total of 100 spectra were simulated for each isotope and 100 spectra were simulated of only background. Each spectrum was simulated with parameters randomly sampled from the simple or complete parameter set.

\begin{table}[H]
\centering
\caption{Range of parameters used for the simple dataset.}
\label{table:hyperparameter_dataset_easy_parameters}
\begin{tabular}{ccc}
% \cline{2-3}
\hline
Hyperparameter & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Source-Detector Distance {[}cm{]}} & 175 & N/A \\ 
\multicolumn{1}{c}{Source-Detector Height {[}cm{]}} & 100 & N/A \\ 
\multicolumn{1}{c}{FWHM 662 keV {[}s{]}} & 7.5 & N/A \\ 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Shielding\\ (Percent 200 keV Attenuated)\end{tabular}} & 0\%, 20\% & Uniform \\ 
\multicolumn{1}{c}{Integration Time {[}s{]}} & 60 - 600 & Log-Uniform \\ 
\multicolumn{1}{c}{Calibration - Offset (channels)} & 0 - 10 & Uniform \\ 
\multicolumn{1}{c}{Calibration - Gain} & 0.9 - 1.1 & Uniform \\ 
\multicolumn{1}{c}{Signal to Background Ratio} & 0.5 - 2.0 & Uniform \\ 
\multicolumn{1}{c}{Background Counts Per Second} & 200 & Poisson \\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Range of parameters used for the complete dataset.}
\label{table:hyperparameter_dataset_full_parameters}
\begin{tabular}{ccc}
% \cline{2-3}
\hline
Hyperparameter & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Source-Detector Distance {[}cm{]}} & 50, 175, 300 & Uniform \\ 
\multicolumn{1}{c}{Source-Detector Height {[}cm{]}} & 50, 100, 150 & Uniform \\ 
\multicolumn{1}{c}{FWHM 662 keV {[}s{]}} & 7.0, 7.5, 8.0 & Uniform \\ 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Shielding\\ (Percent 200 keV Attenuated)\end{tabular}} & 0\%, 20\%, 40\%, 60\% & Uniform \\ 
\multicolumn{1}{c}{Integration Time {[}s{]}} & 10 - 3600 & Log-Uniform \\ 
\multicolumn{1}{c}{Calibration - Offset (channels)} & 0 - 10 & Uniform \\ 
\multicolumn{1}{c}{Calibration - Gain} & 0.8 - 1.2 & Uniform \\ 
\multicolumn{1}{c}{Signal to Background Ratio} & 0.1 - 3.0 & Uniform \\ 
\multicolumn{1}{c}{Background Counts Per Second} & 200 & Poisson \\ \hline
\end{tabular}
\end{table}


\section{Hyperparameter Search Results}

To determine an optimum combination of hyperparameters, a random hyperparameter search was performed. The average testing set error from 5-fold cross validation was recorded for each network and both datasets. An early stopping patience of 20 epochs was used to train 256 models. The maximum number of epochs was set to 200 and training ended if the F1 score in the test set did not rise above 10$\%$ after 10 epochs. The results of the hyperparameter searches are shown using random efficiency curves and by comparing parameter values versus average testing set F1 scores from the 5-fold cross validation. Random efficiency experiment curves indicate the quality of the hyperparameter search space and allow for reproducibility. Analyzing how the parameter values change the F1 score indicates which parameters are important. Hyperparameter bounds are based on previous published experiments as well as literature recommendations \cite{kamuda2017, kamuda2018, Bengio2018}.

% A sharper efficiency curve shows that many hyperparameter combinations perform well, whereas 

% The shape of this curve indicates the frequency of good models under random search, and quantifies the relative volumes (in search space) of the various levels of performance - Bergstra2012a


% These also allow for researchers who want to fit models to this dataset to have a performance benchmark. If you wanted to compare random hyperparameter search to more intelligent methods, you could compare using this.

\subsection{Dense Architecture}

Architecture and training hyperparamters for the DNN are shown in Table \ref{table:hyperparameter_dataset_parameters_DNN}. The number of nodes in each layers decrease for each subsequent layer. The input scaling is read left-to-right. For example, the $sqrt-max$ scaling would first take the square root of the each channel in the spectrum and then normalized the spectrum by its maximum value. The $L1-norm$ normalizes a spectrum by it's L1 norm. The $log1p$ function is the base 10 logarithm of the input plus one.

\begin{table}[H]
\centering
\caption{Range of hyperparameters explored for the DNN.}
\label{table:hyperparameter_dataset_parameters_DNN}
\begin{tabular}{ccc}
% \cline{2-3}
\hline
Hyperparameter & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Number of Layers} & 1 - 3 & Uniform \\
\multicolumn{1}{c}{Nodes in Layer} & 2$^{5}$ - 2$^{10}$ & Power of Two \\
\multicolumn{1}{c}{Initial Learning Rate} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
\multicolumn{1}{c}{L2 Regularization Strength} & 10$^{-2}$ - 10$^{0}$ & Log-Uniform \\
\multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
\multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{10}$ & Power of Two \\
\multicolumn{1}{c}{Activation Function} & relu, tanh & Uniform \\
\multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\ sqrt-L1 norm,\\ log1p, log1p-max,\\ log1p-L1 norm\end{tabular} & Uniform \\ \hline
\end{tabular}
\end{table}



Random efficiency experiment curves for the DNN trained on the simple and complete datasets are shown in Figures \ref{fig:random_hp_search_dnn_easy} and \ref{fig:random_hp_search_dnn_full}. A large number of hyperparameter combinations did not reduce their F1 score in the training set above the 10\% threshold after 10 epochs, ending their training early. As expected, the complete dataset took more trials to obtain good performance. The median F1 score in the simple dataset begins to asymptote after an experiment size of 16 while the F1 score in the complete dataset does not asymptote. This indicated that when applying DNN's to more difficult problems in gamma-ray spectroscopy either wider hyperparameter ranges need to be explored, more advanced hyperparameter search strategies need to be employed, or that due to their structure DNN's are not well suited to perform tasks in gamma-ray spectroscopy.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_dnn_easy}
	\caption{Random hyperparameter search efficiency curves for the DNN using the simplified dataset.}
	\label{fig:random_hp_search_dnn_easy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_dnn_full}
	\caption{Random hyperparameter search efficiency curves for the DNN using the complete dataset.}
	\label{fig:random_hp_search_dnn_full}
\end{figure}


Figure \ref{fig:dense_hyperparameters_f1_score} shows the distribution of average testing set F1 score from the 5-folds cross validation for each hyperparameter. Conclusions based on these will suffer from a small sample size because few networks trained on the complete dataset achieved high F1 scores. General trends in the simple dataset are more representative of actual performance.

Sub-figure \ref{fig:dnn_learning_rate} shows that a learning rate less than 10$^{-2}$ should be used for future hyperparameter searches. This figure also shows that the complete dataset prefers a slower learning rate, with best performing networks using learning rates below 10$^{-3.5}$. Smaller learning rates should also be explored in future searches, if computationally feasible.

Sub-figure \ref{fig:dnn_dropout} shows that the dropout rate has a less obvious performance cutoff compared to the learning rate. This is likely because the dropout value does not have a large effect on training.

Sub-figure \ref{fig:dnn_batch_size} shows that the simple dataset works well in a variety of mini-batch sizes, while the complete dataset may require a larger mini-batch size.

Sub-figures \ref{fig:dnn_dense_nodes_total} and \ref{fig:dnn_dense_layers_total} show the effect of model capacity on performance. Model capacity is comparable to the number of free parameters of a model; too much capacity can lead to overfitting and too little capacity may be insufficient to fit complex data. Overfitting can be seen in the performance drop in three dense layers. Additional layers are unnecessary for problems of this difficulty with the hyperparameters explored. For the complete dataset a significant number of networks perform well with a total number of nodes between 250 and 1000. The simple dataset performed well with a wide range of total nodes.

Sub-figures \ref{fig:dnn_scaler} shows the effect of feature preprocessing on performance. L1 normalization performs very poorly due to the numerical scale of the features. Gradients computed with learning rates explored by the hyperparameter search are too small to significantly update the weights because the features are on the order 10$^{-3}$. Features explored by the other methods are typically between 10$^{0}$ - 10$^{2}$. Scaling spectra using the $sqrt$ of their features makes networks perform well for both datasets. In particular, using $sqrt-max$ scaling moves the most models to high F1 scores.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_learning_rate.png}
        %  \caption{Learning Rate}
         \caption{}
         \label{fig:dnn_learning_rate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dropout.png}
        %  \caption{Dropout Rate}
         \caption{}
         \label{fig:dnn_dropout}
     \end{subfigure}

     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_batch_size.png}
        %  \caption{Batch Size}
         \caption{}
         \label{fig:dnn_batch_size}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dense_layers_total.png}
        %  \caption{Total Dense Layers}
         \caption{}
         \label{fig:dnn_dense_layers_total}
     \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_l2_reg.png}
        %  \caption{L2 Regularization Scale}
         \caption{}
         \label{fig:dnn_l2_reg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dense_nodes_total.png}
        %  \caption{Total Nodes}
         \caption{}
         \label{fig:dnn_dense_nodes_total}
     \end{subfigure}     

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dnn_scaler.png}
        % \caption{Feature Scaling}
         \caption{}
        \label{fig:dnn_scaler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dnn_activation_function.png}
        % \caption{Activation Function}
         \caption{}
        \label{fig:dnn_activation_function}
    \end{subfigure}

        \caption{Effect of hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset.}
        \label{fig:dense_hyperparameters_f1_score}
\end{figure}


% \subsubsection{Hyperparameter Search Results - Autoencoders}

% Now that we know optimized autoencoder architectures for the DNN, we need to optimize training hyperparameters. 

% https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/

% \cite{Ranzato2007} Fig 6 shows you can freeze the unsupervised feature extraction network and update the classifier if you have enough data.

% Sparse denoising autoencoders are included in in this work as feature extraction and dimension reduction techniques. Both autoencoders employ regularization techniques to ensure useful representations are learned. Both models includes $l1$ activity regularization as a method to induce sparsity on the networks activations, which increases generalization \cite{Goodfellow-et-al-2016}. 

% Show how well autoencoders worked at spectrum reconstruction and background subtraction. See if there's a large difference between doing background subtraction and 
\begin{table}[H]
\centering
\caption{Optimum DNN hyperparameter combination found for the simple and complete dataset.}
\label{table:hyperparameter_opt_parameters_DNN}
\begin{tabular}{ccc}
\hline
\multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
 & Simple Dataset & Complete Dataset \\ \hline
Dense Layer Structure & (64) & (32) \\
Initial Learning Rate & 0.00089 & 0.00024 \\
L2 Regularization Strength & 0.13 & 0.0097 \\
Dropout Frequency & 0.023 & 0.16 \\
Batch Size & 256 & 16 \\
Activation Function & relu & relu \\
Input Scaling & sqrt & sqrt-max \\ \hline
\end{tabular}
\end{table}





\subsection{Convolution Architecture}

Architecture and training hyperparamters for the CNN are shown in Table \ref{table:hyperparameter_dataset_parameters_CNN}. Note, as with the DNN the number of nodes in the dense layers decreased with each subsequent layer. A smaller range of batch sizes was searched in the CNN compared to the DNN due to computational constraints.

\begin{table}[H]
\centering
\caption{Range of hyperparameters explored for the CNN.}
\label{table:hyperparameter_dataset_parameters_CNN}
\begin{tabular}{ccc}
% \cline{2-3}
\hline
Hyperparameter & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Number of Filter Kernels} & \begin{tabular}[c]{@{}c@{}}(4)   (8)   (16)    (32)\\ (4, 8)  (8, 16)  (16, 32)\\ (4, 8, 16)   (8, 16, 32)\end{tabular} & Uniform \\
\multicolumn{1}{c}{Filter Kernel Length} & 2, 4, 8, 16 & Uniform \\
\multicolumn{1}{c}{Pooling size} & 2, 4, 8, 16 & Uniform \\
\multicolumn{1}{c}{Number of Dense Layers} & 1 - 3 & Uniform \\
\multicolumn{1}{c}{Nodes in Dense Layers} & 10 - 1000 & Log-Uniform \\
\multicolumn{1}{c}{Initial Learning Rate} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
\multicolumn{1}{c}{L2 Regularization Strength} & 10$^{-3}$ - 10$^{0}$ & Log-Uniform \\
\multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
\multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{6}$ & Power of Two \\
\multicolumn{1}{c}{Activation Function} & tanh, relu & Uniform \\
\multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\sqrt-L1 Norm,\\ log1p-None, log1p-max,\\ log1p-L1 Norm\end{tabular} & Uniform \\ \hline
\end{tabular}
\end{table}


Random efficiency experiment curves for the CNN trained on the simple and complete datasets are shown in Figures \ref{fig:random_hp_search_cnn_easy} and \ref{fig:random_hp_search_cnn_full}. Similar to the dense network, a large number of hyperparameter combinations did not reduce their F1 score in the training set in enough epochs and their training was ended early. Contrary to the DNN, the median F1 score in both datasets smoothly increased with additional trials. Both datasets achieved an F1 score above 90$\%$. This demonstrates that the hyperparameter bounds used for both problems are well suited to the problems and that the CNN architectures explored in this search have more potential than the DNN architectures for gamma-ray spectroscopy. As with the DNN, the final F1 scores on the simplified dataset are higher than the final F1 scores on the complete dataset.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_cnn_easy}
	\caption{Random hyperparameter search efficiency curves for the CNN using the simplified dataset.}
	\label{fig:random_hp_search_cnn_easy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_cnn_full}
	\caption{Random hyperparameter search efficiency curves for the CNN using the complete dataset.}
	\label{fig:random_hp_search_cnn_full}
\end{figure}


Figure \ref{fig:cnn_hyperparameters_f1_score} shows the distribution of hyperparameters searched for the CNN and their average testing set F1 score from the 5-folds cross validation. Similar to the DNN, few networks trained on the complete dataset achieved high F1 scores. This means conclusions based on these suffer from a small (albeit larger than the DNN) sample size.

The CNN's training performance was largely agnostic to many hyperparameters, including those associated with the dense part of the CNN. Figure \ref{fig:cnn_hyperparameters_f1_score} shows that both datasets train similarly over a wide range of learning rates below 10$^{-2}$, dense layer dropout rate below 0.6, L2 regularization scales below 10$^{-1}$, between 50 - 200 total dense nodes, and both mini-batch sizes, 

The CNN training performance on the complete dataset was sensitive to hyperparameters related to model capacity. The total number of convolutional layers had the largest affect on the performance of the complete dataset. Because of the additional complexity in the complete dataset, deeper networks - which extract more abstract features - are necessary for good performance. CNNs with additional convolutional layers should be explored for problems of similar complexity. Additional dense layers did not provide the same boost in performance. The simple dataset performed well with a large range of total dense layers while the complete dataset preferred fewer layers, achieving the best F1 score with a single layer. A high number of dense layers overtrain on more complicated datasets. 

Performance was also sensitive to other convolutional hyperparameters. Both datasets worked well with each convolutional pooling size, but the complete dataset worked best when the convolutional pooling size was largest. Larger pooling sizes add more shift and scale invariance, which is more important in the complete dataset due to a wider range of detector gain settings. Similarly, larger convolutional kernel lengths were required for good performance in the complete dataset. Larger kernels incorporate information in a larger range of channels. Larger kernels may be necessary to incorporate photopeaks and Compton continua, where smaller filters may be sufficient to identify photopeaks alone. Identifying the photopeaks may be enough to identify isotopes in the simple dataset because there are fewer confounding variables. The preference for depth in the convolutional part of the CNN was also seen in the number of convolutional filters.

The effect of scaling on performance was similar to the DNN. Using $sqrt$ scaling yielded the best performing networks, especially when tied with $max$ normalization. While $log1p-L1$ performed very poorly, $sqrt-L1$ yielded comparable performance to the other scaling strategies - despite the discrepancy in feature magnitude. This is likely due to the fact that the CNN extracts scale invariant features (like photopeak locations) from the input signal.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_learning_rate.png}
         \caption{Learning Rate}
         \label{fig:cnn_learning_rate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_dropout.png}
         \caption{Dropout Rate}
         \label{fig:cnn_dropout}
     \end{subfigure}

     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_batch_size.png}
         \caption{Batch Size}
         \label{fig:cnn_batch_size}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_pooling_size.png}
         \caption{CNN Pooling Size}
         \label{fig:cnn_pooling_size}
     \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_l2_reg.png}
         \caption{L2 Regularization Scale}
         \label{fig:cnn_l2_reg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_dense_nodes_total.png}
         \caption{Total Dense Nodes}
         \label{fig:cnn_dense_nodes_total}
     \end{subfigure}  
     
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_kernel_length.png}
         \caption{Convolution Kernel Length}
         \label{fig:cnn_kernel_length}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_num_conv_layers.png}
         \caption{Total Convolution Layers}
         \label{fig:cnn_num_conv_layers}
     \end{subfigure}  
    \caption{Effect of CNN hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset.}
\end{figure}

\begin{figure} \ContinuedFloat
    \centering
    
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \raisebox{-0.49cm}{
        \includegraphics[width=\textwidth]{images/cnn_scaler.png}
        % }
        \caption{Feature Scaling}
        \label{fig:cnn_scaler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/cnn_filter_structure.png}
        \caption{Number of Convolutional Filters}
        \label{fig:cnn_filter_structure}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/cnn_dense_layers_total.png}
        \caption{Total Dense Layers}
        \label{fig:cnn_dense_layers_total}
    \end{subfigure}

        \caption{Effect of CNN hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset (cont.).}
        \label{fig:cnn_hyperparameters_f1_score}
\end{figure}

\begin{table}[H]
\centering
\caption{Optimum CNN hyperparameter combination found for the simple and complete dataset.}
\label{table:hyperparameter_opt_parameters_CNN}
\begin{tabular}{ccc}
\hline
\multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
 & Simple Dataset & Complete Dataset \\ \hline
Filter Kernel Structure & (8, 16, 32) & (8, 16, 32) \\
Filter Kernel Length & 8 & 16 \\
Pooling size & 2 & 16 \\
Dense Layer Structure & (64) & (16) \\
Initial Learning Rate & 0.010 & 0.00024 \\
L2 Regularization Strength & 0.0016 & 0.0097 \\
Dropout Frequency & 0.32 & 0.16 \\
Batch Size & 16 & 16 \\
Activation Function & relu & relu \\
Input Scaling & sqrt-max & sqrt-max \\ \hline
\end{tabular}
\end{table}

% \begin{table}[H]
% \centering
% \caption{Range of hyperparameters explored for the CNN.}
% \label{table:hyperparameter_dataset_parameters_CNN}
% \begin{tabular}{ccc}
% % \cline{2-3}
%  & Hyperparameter Range & Sampling \\ \hline
% \multicolumn{1}{c}{} & \begin{tabular}[c]{@{}c@{}}(4)   (8)   (16)    (32)\\ (4, 8)  (8, 16)  (16, 32)\\ (4, 8, 16)   (8, 16, 32)\end{tabular} & Uniform \\
% \multicolumn{1}{c}{} & 2, 4, 8, 16 & Uniform \\
% \multicolumn{1}{c}{} & 2, 4, 8, 16 & Uniform \\
% \multicolumn{1}{c}{} & 1 - 3 & Uniform \\
% \multicolumn{1}{c}{} & 10 - 1000 & Log-Uniform \\
% \multicolumn{1}{c}{} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
% \multicolumn{1}{c}{} & 10$^{-3}$ - 10$^{0}$ & Log-Uniform \\
% \multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
% \multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{6}$ & Power of Two \\
% \multicolumn{1}{c}{Activation Function} & tanh, relu & Uniform \\
% \multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\sqrt-L1 Norm,\\ log1p-None, log1p-max,\\ log1p-L1 Norm\end{tabular} & Uniform \\
% \end{tabular}
% \end{table}

\subsection{Autoencoder Architectures} \label{section_autoencoder_archetectures}

Without an autoencoder, a single ANN has to learn multiple tasks to identify isotopes. An ANN would have to simultaneously identify the detector calibration, background signal, and source signal. By pretraining each network using an autoencoder to reconstruct a background-subtracted spectrum, the task of isotope identification is simplified for the ANN. To explore if using pretrained models results in more accurate identifications, a DAE and CAE were trained using the model parameters found in the hyperparameter search.

% (Tables \ref{table:hyperparameter_dataset_easy_parameters} and \ref{table:hyperparameter_dataset_full_parameters})
Pretraining was performed using a dataset of 10000 samples generated using the simple and complete dataset parameters. Because of the implicit regularization of the undercomplete encoding and the denoising background subtracting process, both networks do not use additional regularization when training. The initial learning rate of each autoencoder was set to 10$^{-5}$.

% This section will explain the hyperparameter structures searched. The autoencoder used is a undercomplete denoising autoencoder \cite{Goodfellow-et-al-2016}. Because the denoising operation performs regularization implicitly, no additional regularization is added to the autoencoders. 

% The autoencoder structures are found using a grid search. The autoencoders that reproduce the background subtracted signal the best are used in a dense random hyperparameter search as the signal preprocessing.


% The DAE structure is based on the dense hyperparameter search. The DAE trained on both datasets uses the same parameters as Table \ref{table:hyperparameter_dataset_parameters_DNN} without regularization and with an initial learning rate set to 10$^{-5}$. The DAE does not use L2 regularization or dropout due to the implicit regularization of the undercomplete encoding and the denoising background subtracting process.

% \begin{table}[H]
% \centering
% \caption{Optimum DNN hyperparameter combination found for the simple and complete dataset.}
% \label{table:hyperparameter_dataset_parameters_DNN}
% \begin{tabular}{ccc}
% \multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
%  & Simple Dataset & Complete Dataset \\ \hline
% Dense Layer Structure & (64) & (32) \\
% Initial Learning Rate & 10$^{-5}$ & 10$^{-5}$ \\
% Batch Size & 256 & 16 \\
% Activation Function & relu & relu \\
% Input Scaling & sqrt & sqrt-max
% \end{tabular}
% \end{table}

% The CAE structure is based on the convolutional hyperparameter search. The CAE's trained on both datasets use the same parameters as Table \ref{table:hyperparameter_dataset_parameters_CNN} without regularization and with an initial learning rate set to 10$^{-5}$.


% \section{Summary of Final Model Architectures}


% This section discusses performance differences between the DNN, CNN, CAE-DNN, and DAE-DNN. These differences will be based on the final testing set error for all models.  





