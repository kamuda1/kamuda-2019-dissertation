\chapter{Machine Learning Models Explored}

This section describes the process of choosing the architectural and regularization hyperparamers for the dense and 1D convolutional models explored in this work. This section also describes the datasets used to tune the hyperparameters.

% Dense and convolutional models are explored for two different reasons. 

\subsection{Training Templates Overview}

The datasets used to train the models are created using templates of simulated gamma-ray spectra without Poisson noise. A one-dimensional particle transport code developed at Sandia National Laboratory, GADRAS-DRF (Gamma Detector Response and Analysis Software - Detector Response Function), was used to generate these templates. This simulation code can be used to model physical conditions that affect spectra. For example, spectra change due to changes in measurement geometry. These changes include source-to-detector distance and the distance of both the detector and source from objects in an environment. To demonstrate this effect, $^{60}$Co spectra were simulated at various source-to-detector distances, Figure \ref{fig:sim_spectra_distance_comparison}, and heights off ground, Figure \ref{fig:sim_spectra_height_comparison}. These parameters change the the shape of the Compton continuum, the peak-to-total ratio, and the magnitude of the backscatter peak. Another parameter that changes the spectrum is the Gaussian energy broadening of the photopeaks. Due to manufacturing differences, each detector has a different amount of energy broadening. Examples of $^{60}$Co spectra with different broadening parameters are shown in Figure \ref{fig:sim_spectra_FWHM_comparison}.


\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_distance_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated at various source-detector distances.}
\label{fig:sim_spectra_distance_comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_height_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated at various source-detector heights off the ground.}
\label{fig:sim_spectra_height_comparison}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/sim_spectra_FWHM_comparison}
\caption{Comparison of a $^{60}$Co spectrum simulated with various FWHM parameters.}
\label{fig:sim_spectra_FWHM_comparison}
\end{figure}




% To incorporate these changes, templates simulated at different distances are included in the dataset. These distances start at 30cm, which is the distance at which a 1 uCi source will have an activity of about 400 cps on a 2 inch diameter detector. This is about twice the expected activity of background. 

% Changes in calibration due to temperature shifts are also considered. Due to the relatively large magnitude in calibration shifts due to temperature shifts from -5 C to 40 C \cite{CASANOVAS2012588}, it is expected incorporating these additions will also make the algorithm robust against calibration.

The template parameters used in this study are shown in Table \ref{table:all_fixed_simulation_parameters}. These parameters are based on handheld RIID scenarios. The source-detector height off ground are sampled between average shin and arm heights. The source-detector distance range corresponds to expected standoff distances. The areal density of each material corresponds to a 20$\%$, 40$\%$, 60$\%$, and 80$\%$ attenuation of a 200 keV photon. The photon energy was chosen because it is near the 186 keV energy of the most important $^{235}$U photopeak. A set of unshielded templates is also included. The FWHM range was chosen based on reported FWHM values for a NaI(Tl) detector (see section \ref{subsection_energy_resolution}). Background locations were chosen based on their geographic and geologic differences.

In addition to the parameters simulated by GADRAS-DRF, parameters are included to perform data augmentation. These parameters are shown in Table \ref{table:all_variable_simulation_parameters}.

\begin{table}[H]
\centering
\caption{Fixed GADRAS-DRF simulation parameters.}
\begin{tabular}{cc}
Parameter & Values \\ \hline
\begin{tabular}[c]{@{}c@{}}source-detector height \\ off ground {[}cm{]}\end{tabular} & 50.0, 75.0, 100.0, 125.0, 150.0 \\ 
\begin{tabular}[c]{@{}c@{}}source-detector\\ distance {[}cm{]}\end{tabular} & \multicolumn{1}{c}{50.0, 112.5, 175.0, 237.5, 300.0} \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of \\ solid aluminum {[}g/cm\textasciicircum{}2{]}\end{tabular} & 1.82, 4.18, 7.49, 13.16 \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of\\  solid iron {[}g/cm\textasciicircum{}2{]}\end{tabular} & 1.53, 3.5, 6.28, 11.02 \\ 
\begin{tabular}[c]{@{}c@{}}Areal density of \\ solid lead {[}g/cm\textasciicircum{}2{]}\end{tabular} & 0.22, 0.51, 0.92, 1.61 \\ 
\begin{tabular}[c]{@{}c@{}}FWHM at\\  662 keV {[}\%{]}\end{tabular} & \multicolumn{1}{c}{6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0} \\ 
\begin{tabular}[c]{@{}c@{}}Background\\ Location \end{tabular} & \begin{tabular}[c]{@{}c@{}}Albuquerque, Atlanta, Austin,\\ Chicago , Knoxville , Miami\end{tabular}
\end{tabular}
\label{table:all_fixed_simulation_parameters}
\end{table}

\begin{table}[H]
\centering
\caption{Default data augmentation parameters.}
\begin{tabular}{c}
Parameter \\ \hline
integration time \\ 
background counts per second \\ 
Signal to Background ratio \\ 
Calibration - Offset \\ 
Calibration - Gain \\ 
\end{tabular}
\label{table:all_variable_simulation_parameters}
\end{table}

% \begin{table}[H]
% \centering
% \caption{Default data augmentation parameters.}
% \begin{tabular}{ccc}
% Parameter & Values & Sampling \\ \hline
% integration time & 60 - 600 & uniform \\ 
% background counts per second & 200 & Poisson \\ 
% Signal to Background ratio & 0.5 - 2 & uniform \\ 
% 0th order calibration & 0 - 10 & uniform \\ 
% 1st order calibration & 0.83 - 1.16 & uniform \\ 
% \end{tabular}
% \label{table:all_variable_simulation_parameters}
% \end{table}


\section{Datasets Used for the Hyperparameter Search}

To investigate which parameters affect generalization performance, networks were trained using a simple and complete set of template simulation parameters. The parameters used for the simplified dataset are shown in Table \ref{table:hyperparameter_dataset_easy_parameters} and parameters used for the complete dataset are shown in Table \ref{table:hyperparameter_dataset_full_parameters}. Both datasets are comprised of the following isotopes from ANSI N42-34-2006 \cite{ANSI}: $^{241}$Am, $^{133}$Ba, $^{57}$Co, $^{60}$Co, $^{51}$Cr, $^{137}$Cs, $^{152}$Eu, $^{67}$Ga, $^{123}$I, $^{125}$I, $^{131}$I, $^{111}$In, $^{192}$Ir, $^{177m}$Lu, $^{99}$Mo, $^{237}$Np, $^{103}$Pd, $^{239}$Pu, $^{240}$Pu, $^{226}$Ra, $^{75}$Se, $^{153}$Sm, $^{99m}$Tc, $^{201}$Tl, $^{204}$Tl, $^{233}$U, $^{235}$U, $^{238}$U, and $^{133}$Xe.

Because datasets of different complexity often need different architectures and hyperparameters \cite{Bergstra2012}, independent hyperparameter searches were conducted for both datasets. For each dataset, a total of 100 spectra were simulated for each isotope. Each spectrum was simulated with parameters randomly sampled from the simple or complete parameter set.

\begin{table}[H]
\centering
\caption{Range of parameters used for the simple dataset.}
\label{table:hyperparameter_dataset_easy_parameters}
\begin{tabular}{ccc}
% \cline{2-3}
 & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Source-Detector Distance {[}cm{]}} & 175.0 & N/A \\ 
\multicolumn{1}{c}{Source-Detector Height {[}cm{]}} & 100.0 & N/A \\ 
\multicolumn{1}{c}{FWHM 662 keV {[}s{]}} & 7.5 & N/A \\ 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Shielding\\ (Percent 200 keV Attenuated)\end{tabular}} & 0\%, 20\% & Uniform \\ 
\multicolumn{1}{c}{Integration Time {[}s{]}} & 60 - 600 & Log-Uniform \\ 
\multicolumn{1}{c}{Calibration - Offset (channels)} & 0 - 10 & Uniform \\ 
\multicolumn{1}{c}{Calibration - Gain} & 0.9 - 1.1 & Uniform \\ 
\multicolumn{1}{c}{Signal to Background Ratio} & 0.5 - 2.0 & Uniform \\ 
\multicolumn{1}{c}{Background Counts Per Second} & 200 & Poisson \\ 
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Range of parameters used for the complete dataset.}
\label{table:hyperparameter_dataset_full_parameters}
\begin{tabular}{ccc}
% \cline{2-3}
 & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Source-Detector Distance {[}cm{]}} & 50.5, 175.0, 300 & Uniform \\ 
\multicolumn{1}{c}{Source-Detector Height {[}cm{]}} & 50, 100.0, 150 & Uniform \\ 
\multicolumn{1}{c}{FWHM 662 keV {[}s{]}} & 7.0, 7.5, 8.0 & Uniform \\ 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Shielding\\ (Percent 200 keV Attenuated)\end{tabular}} & 0\%, 20\%, 40\%, 60\% & Uniform \\ 
\multicolumn{1}{c}{Integration Time {[}s{]}} & 10 - 3600 & Log-Uniform \\ 
\multicolumn{1}{c}{Calibration - Offset (channels)} & 0 - 10 & Uniform \\ 
\multicolumn{1}{c}{Calibration - Gain} & 0.8 - 1.2 & Uniform \\ 
\multicolumn{1}{c}{Signal to Background Ratio} & 0.1 - 3.0 & Uniform \\ 
\multicolumn{1}{c}{Background Counts Per Second} & 200 & Poisson \\ 
\end{tabular}
\end{table}


\section{Hyperparameter Search Results}

To determine an optimum combination of hyperparameters, a random hyperparameter search over hyperparameters was performed. For each network and both datasets, the average testing set error from 5-fold cross validation was recorded. An early stopping patience of 20 epochs was used to train 256 models. To reduce wall-clock time, the maximum number of epochs was set to 200 and training ended if the F1 score in the test set did not rise above 10$\%$ after 10 epochs. The results of the hyperparameter searches are shown using random efficiency curves and by comparing parameter values versus average testing set F1 scores from the 5-fold cross validation. Random efficiency curves indicate the quality of the hyperparameter search space and allows for reproducibility. Analyzing how the parameter values change the F1 score gives us an idea of which parameters are important. Hyperparameter bounds are based on previous published and unpublished experiments as well as recommendations from literature \cite{Bengio2018}.

% A sharper efficiency curve shows that many hyperparameter combinations perform well, whereas 

% The shape of this curve indicates the frequency of good models under random search, and quantifies the relative volumes (in search space) of the various levels of performance - Bergstra2012a


% These also allow for researchers who want to fit models to this dataset to have a performance benchmark. If you wanted to compare random hyperparameter search to more intelligent methods, you could compare using this.

\subsection{Dense Architecture}

Architecture and training hyperparamters for the DNN are shown in Table \ref{table:hyperparameter_dataset_parameters_DNN}. Note, the number of nodes in each layers was made to decrease for each subsequent layer. The input scaling is read left-to-right. For example, the sqrt-max scaling would first take the square root of the each channel in the spectrum and then normalized the spectrum by its maximum value. The L1 norm normalizes a spectrum by its L1 norm. The log1p function is the base 10 logarithm of the input plus one.

\begin{table}[H]
\centering
\caption{Range of hyperparameters explored for the DNN.}
\label{table:hyperparameter_dataset_parameters_DNN}
\begin{tabular}{ccc}
% \cline{2-3}
Hyperparameter & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Number of Layers} & 1 - 3 & Uniform \\
\multicolumn{1}{c}{Nodes in Layer} & 2$^{5}$ - 2$^{10}$ & Power of Two \\
\multicolumn{1}{c}{Initial Learning Rate} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
\multicolumn{1}{c}{L2 Regularization Strength} & 10$^{-2}$ - 10$^{0}$ & Log-Uniform \\
\multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
\multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{10}$ & Power of Two \\
\multicolumn{1}{c}{Activation Function} & relu, tanh & Uniform \\
\multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\ sqrt-L1 norm,\\ log1p, log1p-max,\\ log1p-L1 norm\end{tabular} & Uniform \\
\end{tabular}
\end{table}



Random efficiency experiment curves for the DNN trained on the simple and complete datasets are shown in Figures \ref{fig:random_hp_search_dnn_easy} and \ref{fig:random_hp_search_dnn_full}. A large number of hyperparameter combinations did not reduce their F1 score in the training set quickly enough and their training was ended early. As expected, the complete dataset took many more trials to obtain good performance. The median F1 score in the simple dataset begins to asymptote after an experiment size of 16 while the F1 score in the complete dataset does not asymptote. This indicated that when applying DNN's to more difficult problems in gamma-ray spectroscopy wider hyperparameter ranges need to be explored, more advanced hyperparameter search strategies need to be employed, or that DNN's are not well suited to perform tasks in gamma-ray spectroscopy. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_dnn_easy}
	\caption{Random hyperparameter search efficiency curves for the DNN using the simplified dataset.}
	\label{fig:random_hp_search_dnn_easy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_dnn_full}
	\caption{Random hyperparameter search efficiency curves for the DNN using the complete dataset.}
	\label{fig:random_hp_search_dnn_full}
\end{figure}


Figure \ref{fig:dense_hyperparameters_f1_score} shows the distribution of hyperparameters searched and their average testing set F1 score from the 5-folds cross validation. Because few networks trained on the complete dataset achieved high F1 scores, conclusions based on these will suffer from a small sample size. General trends in the simple dataset are more representative of actual performance. 

Sub-figure \ref{fig:dnn_learning_rate} shows that a learning rate less than 10$^{-2}$ should be used for future hyperparameter searches. This figure also shows that the more difficult dataset prefers a slower learning rate, with best performing networks using learning rates below 10$^{-3.5}$. Smaller learning rates should also be explored in future searches, if computationally feasible.

Sub-figure \ref{fig:dnn_dropout} shows that a dropout rate has less obvious performance cutoff than the learning rate. This is likely because the dropout value does not have a large effect on training.

Sub-figure \ref{fig:dnn_batch_size} shows that the simple dataset works well in a variety of mini-batch sizes, while the complete dataset may require a larger mini-batch size.

Sub-figures \ref{fig:dnn_dense_nodes_total} and \ref{fig:dnn_dense_layers_total} show the effect of model capacity on performance. Model capacity is comparable to the number of free parameters of a model; too much capacity can lead to overfitting and too little capacity may be insufficient to fit complex data. Overfitting can be seen in the performance drop in three dense layers. Additional layers are unnecessary for problems of this difficulty with the hyperparameters explored. For the complete dataset a significant number of networks perform well with a total number of nodes between 250 and 1000. The simple dataset performed well with a wide range of total nodes.

Sub-figures \ref{fig:dnn_scaler} shows the effect of feature preprocessing on performance. L1 normalization performs very poorly due to the numerical scale of the features. Because the features are on the order 10$^{-3}$, gradients computed with learning rates explored by the hyperparameter search are too small to significantly update the weights. Features explored by the other methods are typically between 10$^{0}$ - 10$^{2}$. Scaling spectra using the $sqrt$ of their features makes networks perform well for both datasets. In particular, using $sqrt-max$ scaling appears to move the most number of models to good F1 scores.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_learning_rate.png}
         \caption{Learning Rate}
         \label{fig:dnn_learning_rate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dropout.png}
         \caption{Dropout Rate}
         \label{fig:dnn_dropout}
     \end{subfigure}

     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_batch_size.png}
         \caption{Batch Size}
         \label{fig:dnn_batch_size}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dense_layers_total.png}
         \caption{Total Dense Layers}
         \label{fig:dnn_dense_layers_total}
     \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_l2_reg.png}
         \caption{L2 Regularization Scale}
         \label{fig:dnn_l2_reg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_dense_nodes_total.png}
         \caption{Total Nodes}
         \label{fig:dnn_dense_nodes_total}
     \end{subfigure}     

     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dnn_scaler.png}
         \caption{Feature Scaling}
         \label{fig:dnn_scaler}
     \end{subfigure}

        \caption{Effect of hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset.}
        \label{fig:dense_hyperparameters_f1_score}
\end{figure}


% \subsubsection{Hyperparameter Search Results - Autoencoders}

% Now that we know optimized autoencoder architectures for the DNN, we need to optimize training hyperparameters. 

% https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/

% \cite{Ranzato2007} Fig 6 shows you can freeze the unsupervised feature extraction network and update the classifier if you have enough data.

% Sparse denoising autoencoders are included in in this work as feature extraction and dimension reduction techniques. Both autoencoders employ regularization techniques to ensure useful representations are learned. Both models includes $l1$ activity regularization as a method to induce sparsity on the networks activations, which increases generalization \cite{Goodfellow-et-al-2016}. 

% Show how well autoencoders worked at spectrum reconstruction and background subtraction. See if there's a large difference between doing background subtraction and 
\begin{table}[H]
\centering
\caption{Optimum DNN hyperparameter combination found for the simple and complete dataset.}
\label{table:hyperparameter_dataset_parameters_DNN}
\begin{tabular}{ccc}
\multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
 & Simple Dataset & Complete Dataset \\ \hline
Dense Layer Structure & (64) & (32) \\
Initial Learning Rate & 0.00089 & 0.00024 \\
L2 Regularization Strength & 0.13 & 0.0097 \\
Dropout Frequency & 0.023 & 0.16 \\
Batch Size & 256 & 16 \\
Activation Function & relu & relu \\
Input Scaling & sqrt & sqrt-max
\end{tabular}
\end{table}





\subsection{Convolution Architecture}

Architecture and training hyperparamters for the CNN are shown in Table \ref{table:hyperparameter_dataset_parameters_CNN}. Note, as with the DNN the number of nodes in the dense layers was made to decrease for each subsequent layer. A smaller range of batch sizes was searched in the CNN compared to the DNN due to computational constraints.

\begin{table}[H]
\centering
\caption{Range of hyperparameters explored for the CNN.}
\label{table:hyperparameter_dataset_parameters_CNN}
\begin{tabular}{ccc}
% \cline{2-3}
 & Hyperparameter Range & Sampling \\ \hline
\multicolumn{1}{c}{Number of Filter Kernels} & \begin{tabular}[c]{@{}c@{}}(4)   (8)   (16)    (32)\\ (4, 8)  (8, 16)  (16, 32)\\ (4, 8, 16)   (8, 16, 32)\end{tabular} & Uniform \\
\multicolumn{1}{c}{Filter Kernel Length} & 2, 4, 8, 16 & Uniform \\
\multicolumn{1}{c}{Pooling size} & 2, 4, 8, 16 & Uniform \\
\multicolumn{1}{c}{Number of Dense Layers} & 1 - 3 & Uniform \\
\multicolumn{1}{c}{Nodes in Dense Layers} & 10 - 1000 & Log-Uniform \\
\multicolumn{1}{c}{Initial Learning Rate} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
\multicolumn{1}{c}{L2 Regularization Strength} & 10$^{-3}$ - 10$^{0}$ & Log-Uniform \\
\multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
\multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{6}$ & Power of Two \\
\multicolumn{1}{c}{Activation Function} & tanh, relu & Uniform \\
\multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\sqrt-L1 Norm,\\ log1p-None, log1p-max,\\ log1p-L1 Norm\end{tabular} & Uniform \\
\end{tabular}
\end{table}


Random efficiency experiment curves for the CNN trained on the simple and complete datasets are shown in Figures \ref{fig:random_hp_search_cnn_easy} and \ref{fig:random_hp_search_cnn_full}. Similar to the dense network, a large number of hyperparameter combinations did not reduce their F1 score in the training set quickly enough and their training was ended early. Interestingly, the median F1 score in both datasets smoothly increased with additional trials. Both datasets achieved an F1 score above 90$\%$. This demonstrates that the hyperparameter bounds used for both problems are well suited to the problems and that CNN's have more potential than DNN's for perform tasks in gamma-ray spectroscopy. As with the DNN, the final F1 scores on the simplified dataset are higher than the final F1 scores on the complete dataset.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_cnn_easy}
	\caption{Random hyperparameter search efficiency curves for the CNN using the simplified dataset.}
	\label{fig:random_hp_search_cnn_easy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{images/random_hp_search_cnn_full}
	\caption{Random hyperparameter search efficiency curves for the CNN using the complete dataset.}
	\label{fig:random_hp_search_cnn_full}
\end{figure}


Figure \ref{fig:cnn_hyperparameters_f1_score} shows the distribution of hyperparameters searched for the CNN and their average testing set F1 score from the 5-folds cross validation. Similar to the DNN, few networks trained on the complete dataset achieved high F1 scores. This means conclusions based on these suffer from a small (albeit larger than the DNN) sample size.

The CNN's training performance was largely agnostic to many hyperparameters, including those associated with the dense part of the CNN. Figure \ref{fig:cnn_hyperparameters_f1_score} shows that both datasets train similarly over a wide range of learning rates blow 10$^{-2}$, dense layer dropout rate below 0.6, L2 regularization scales below 10$^{-1}$, between 50 - 200 total dense nodes, and both mini-batch sizes, 

The CNN training performance on the complete dataset was sensitive to hyperparameters related to model capacity. The total number of convolutional layers had the largest affect on the performance of the complete dataset. Because of the additional complexity in the complete dataset, deeper networks - which extract more abstract features - are necessary for good performance. CNNs with additional convolutional layers should be explored for similar problems. Interestingly, additional dense layers did not provide the same boost in performance. The simple dataset performed well with a large range of total dense layers while the complete dataset preferred fewer layers, achieving the best F1 score with a single layer. This may be show that too many dense layers leads to overtraining on more complicated datasets. 

Performance was also sensitive to other convolutional hyperparameters. Both datasets worked well with each convolutional pooling size, but the complicated dataset worked best when the convolutional pooling size was largest. Larger pooling sizes add more shift and scale invariance, which is more important in the complete dataset due to it's wider range of detector gain settings. Similarly, larger convolutional kernel lengths were were required for good performance in the complete dataset. Larger kernels incorporate information in a larger range of channels. Larger kernels may be necessary to incorporate photopeaks and Compton continua, where smaller filters may be sufficient to identify photopeaks alone. Identifying the photopeaks may be enough to identify isotopes in the simple dataset, but not the more complicated dataset where photopeaks are degraded and changed by a wider range of shielding, source-detector distances, source-detector heights, and signal to background ratios. The preference for depth in the convolutional part of the CNN was also seen in the number of convolutional filters.

The effect of scaling on performance was similar to the DNN. Using $sqrt$ scaling yielded the best performing networks, especially when tied with $max$ normalization. Interestingly, while $log1p-L1$ performed very poorly, $sqrt-L1$ yielded comparable performance to the other scaling strategies - despite the discrepancy in feature magnitude. This is likely due to the fact that the CNN is extracting scale invariant features (like photopeak locations) from the input signal.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_learning_rate.png}
         \caption{Learning Rate}
         \label{fig:cnn_learning_rate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_dropout.png}
         \caption{Dropout Rate}
         \label{fig:cnn_dropout}
     \end{subfigure}

     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_batch_size.png}
         \caption{Batch Size}
         \label{fig:cnn_batch_size}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_pooling_size.png}
         \caption{CNN Pooling Size}
         \label{fig:cnn_pooling_size}
     \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_l2_reg.png}
         \caption{L2 Regularization Scale}
         \label{fig:cnn_l2_reg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_dense_nodes_total.png}
         \caption{Total Dense Nodes}
         \label{fig:cnn_dense_nodes_total}
     \end{subfigure}  
     
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_kernel_length.png}
         \caption{Convolution Kernel Length}
         \label{fig:cnn_kernel_length}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/cnn_num_conv_layers.png}
         \caption{Total Convolution Layers}
         \label{fig:cnn_num_conv_layers}
     \end{subfigure}  
    \caption{Effect of CNN hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset.}
\end{figure}

\begin{figure} \ContinuedFloat
    \centering
    
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        % \raisebox{-0.49cm}{
        \includegraphics[width=\textwidth]{images/cnn_scaler.png}
        % }
        \caption{Feature Scaling}
        \label{fig:cnn_scaler}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/cnn_filter_structure.png}
        \caption{Number of Convolutional Filters}
        \label{fig:cnn_total_dense_layers}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/cnn_dense_layers_total.png}
        \caption{Total Dense Layers}
        \label{fig:cnn_total_dense_layers}
    \end{subfigure}

        \caption{Effect of CNN hyperparameters on F1 score. Blue circles represent the simple dataset, green crosses are the complete dataset (cont.).}
        \label{fig:cnn_hyperparameters_f1_score}
\end{figure}

\begin{table}[H]
\centering
\caption{Optimum CNN hyperparameter combination found for the simple and complete dataset.}
\label{table:hyperparameter_dataset_parameters_DNN}
\begin{tabular}{ccc}
\multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
 & Simple Dataset & Complete Dataset \\ \hline
Filter Kernel Structure & (8, 16, 32) & (8, 16, 32) \\
Filter Kernel Length & 8 & 16 \\
Pooling size & 2 & 16 \\
Dense Layer Structure & (64) & (16) \\
Initial Learning Rate & 0.010 & 0.00024 \\
L2 Regularization Strength & 0.0016 & 0.0097 \\
Dropout Frequency & 0.32 & 0.16 \\
Batch Size & 16 & 16 \\
Activation Function & relu & relu \\
Input Scaling & sqrt-max & sqrt-max
\end{tabular}
\end{table}

% \begin{table}[H]
% \centering
% \caption{Range of hyperparameters explored for the CNN.}
% \label{table:hyperparameter_dataset_parameters_CNN}
% \begin{tabular}{ccc}
% % \cline{2-3}
%  & Hyperparameter Range & Sampling \\ \hline
% \multicolumn{1}{c}{} & \begin{tabular}[c]{@{}c@{}}(4)   (8)   (16)    (32)\\ (4, 8)  (8, 16)  (16, 32)\\ (4, 8, 16)   (8, 16, 32)\end{tabular} & Uniform \\
% \multicolumn{1}{c}{} & 2, 4, 8, 16 & Uniform \\
% \multicolumn{1}{c}{} & 2, 4, 8, 16 & Uniform \\
% \multicolumn{1}{c}{} & 1 - 3 & Uniform \\
% \multicolumn{1}{c}{} & 10 - 1000 & Log-Uniform \\
% \multicolumn{1}{c}{} & 10$^{-4}$ - 10$^{-1}$ & Log-Uniform \\
% \multicolumn{1}{c}{} & 10$^{-3}$ - 10$^{0}$ & Log-Uniform \\
% \multicolumn{1}{c}{Dropout Frequency} & 0 - 1 & Uniform \\
% \multicolumn{1}{c}{Batch Size} & 2$^{4}$ - 2$^{6}$ & Power of Two \\
% \multicolumn{1}{c}{Activation Function} & tanh, relu & Uniform \\
% \multicolumn{1}{c}{Input Scaling} & \begin{tabular}[c]{@{}c@{}}sqrt, sqrt-max,\\sqrt-L1 Norm,\\ log1p-None, log1p-max,\\ log1p-L1 Norm\end{tabular} & Uniform \\
% \end{tabular}
% \end{table}

\subsection{Autoencoder Architectures}

Without an autoencoder, a single ANN has to learn multiple tasks to identify isotopes. An ANN would have to simultaneously identify the detector calibration, background signal, and source signal. By pretraining each network using an autoencoder to reconstruct a background-subtracted spectrum, the task of isotope identification is simplified for the ANN. To explore if using pretrained models results in more accurate identifications, a DAE and CAE were trained using the model parameters found in the hyperparameter search. 

% This section will explain the hyperparameter structures searched. The autoencoder used is a undercomplete denoising autoencoder \cite{Goodfellow-et-al-2016}. Because the denoising operation performs regularization implicitly, no additional regularization is added to the autoencoders. 

% The autoencoder structures are found using a grid search. The autoencoders that reproduce the background subtracted signal the best are used in a dense random hyperparameter search as the signal preprocessing.


% The DAE structure is based on the dense hyperparameter search. The DAE trained on both datasets uses the same parameters as Table \ref{table:hyperparameter_dataset_parameters_DNN} without regularization and with an initial learning rate set to 10$^{-5}$. The DAE does not use L2 regularization or dropout due to the implicit regularization of the undercomplete encoding and the denoising background subtracting process.

% \begin{table}[H]
% \centering
% \caption{Optimum DNN hyperparameter combination found for the simple and complete dataset.}
% \label{table:hyperparameter_dataset_parameters_DNN}
% \begin{tabular}{ccc}
% \multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Optimum Value} \\
%  & Simple Dataset & Complete Dataset \\ \hline
% Dense Layer Structure & (64) & (32) \\
% Initial Learning Rate & 10$^{-5}$ & 10$^{-5}$ \\
% Batch Size & 256 & 16 \\
% Activation Function & relu & relu \\
% Input Scaling & sqrt & sqrt-max
% \end{tabular}
% \end{table}

% The CAE structure is based on the convolutional hyperparameter search. The CAE's trained on both datasets use the same parameters as Table \ref{table:hyperparameter_dataset_parameters_CNN} without regularization and with an initial learning rate set to 10$^{-5}$.


% \section{Summary of Final Model Architectures}


% This section discusses performance differences between the DNN, CNN, CAE-DNN, and DAE-DNN. These differences will be based on the final testing set error for all models.  





